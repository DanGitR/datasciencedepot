# "Red Flags" or Opportunities?

In a July 2018 blog post, [Red Flags in Data Science Interviews](http://bit.ly/2MLRXDO), Emily Robinson and Jonathan Nolis discuss twelve "red flags" that job candidates can use to evaluate companies which which they are interviewing for data science jobs. The red flags are organized into two categories:

* Red flags on how the data science team runs, and
* Red flags on how they value people.

Robinson and Nolis make a strong point that job candidates should evaluate prospective companies as much as they are being evaluated as candidates. That said, the article is based on an unrealistic underlying assumption: one can find an place to work that has none of these "red flags." On the contrary, every workplace has its challenges. In fact, a company that is recruiting more team members is already admitting that it isn't "ideal" in that it at a minimum, needs more help to meet its customers' needs.

Another way to look at the red flags reviewed in the article is to view them as opportunities where candidates can make a difference, helping the company improve its results while growing one's skills and experience.

# Simplifying Assumptions

We'll start with some simplifying assumptions to ground the remainder of this article.

1. **Companies hire on need:** During the interview process, hiring managers are looking to fill specific gaps in their talent base, and are focused on determining whether a candidate fills these gaps. <br><br>
2. **No company is "ideal":** My personal experience is that every company has areas of improvement, even the company I founded. Therefore, a good fit between company and candidate occurs when a candidate's strengths help address a company's areas of growth / improvement. <br><br>
3. **Data science is hard:** As I wrote in [Value of the Johns Hopkins Data Science Specialization](http://bit.ly/2j3EcCn), data science in real life is all about ambiguous problems, messy data, scores of algorithms to consider, and limited computer resources that challenge the scientist to produce actionable results within an acceptable timeframes and cost structures. <br><br>
4. **People get paid for the "Three Rs:"** Generally speaking, people get paid to manage what I call "the three Rs" of *resources*, *risks*, and *revenue*. A fair amount of data science is related to managing risk -- risk of messy data, risk of insufficient computer resources, risk that models won't have required positive & negative predictive value, etc.

# Seeing the Opportunities in "Red Flags"

Given our simplifying assumptions, the interview process is a pattern matching activity: can we match the candidate's skills and strengths to the needs in a company, where the candidate gets meaningful work and rewards in exchange for her/his best effort?

Each red flag discussed in the original article can now be repurposed with a set of questions to guide a candidate's discussion with prospective employers.

## Red flags on how the data science team requirements

1. **No data engineering or infrastructure:** What is the opportunity to establish / improve the data engineering? What problems do you experience given the organization's current level of maturity? What value might be generated by mitigating these risks? <br><br>
2. **No peer review between data scientists:** How is work currently reviewed within the organization? How might a peer review process improve the quality and consistency of the work product? How might a peer review process increase diversity of skills & experience across the team? <br><br>
3. **No standard set of languages across the team:** How might the team increase its delivery velocity, quality, and value to customers by establishing a standard set of languages? How would we measure this value? <br><br>
4. **No understanding of the [AI hierarchy of needs](http://bit.ly/2KMQsof):** Where is the company on the hierarchy of needs? How does the company's position relate to my skill set? How might I leverage my skills to help the company move up the maturity hierarchy? What skills can I develop by working with colleagues at the company? <br><br>
5. **No version control:** Has the company ever experienced having to rerun an analysis because of a coding defect? Has the company ever deployed an old version of a model over a newer one because a team member overwrote newer code with older code? How frequently does this happen? What is the value of eliminating these types of problems?<br><br>
6. **No clear delineation between people who run reports vs. do analyses:** How is work allocated to various roles in the data science process? How might we improve the organization's effectiveness by increasing team members' focus on the highest value work?  <br><br>

## Red flags on how they value people

1. **A totally non-structured interview process:** How does the organization make decisions about candidates? How does it incorporate data science practices (e.g. objective criteria) in the hiring process? What skills / qualities are valued here, and how might the interviewers observe them if the process is unstructured?<br><br>
2. **No time for your questions:** <br><br>
3. **No coding required in the interview:** <br><br>
4. **No plan for your first few months:** <br><br>
5. **No support for continuing education:** <br><br>
6. **Inconsistent answers between interviewers about the role:** <br><br>  

# Conclusion: Every job is an opportunity to develop character



[Return Home](http://bit.ly/2ouaZTF)

*Â© 2017 - 2018 Leonard M. Greski - copying with attribution permitted*
